# Q-Learning-GridWorld
Reinforcement Learning in a 5Ã—5 Environment

---

ğŸ§  **Learning to Navigate with Q-Learning**  
This is a Mathematica-based project where I implemented **Q-Learning** in a simple **5Ã—5 grid-world**.  
The goal was to see how an agent can discover an optimal path to a target while avoiding obstacles â€” starting from scratch, with no prior knowledge of the environment.  

Instead of black-box frameworks, everything here is **built from the ground up**: state definitions, reward structures, Q-updates, and visualizations.  
The emphasis is on understanding the mechanics of reinforcement learning rather than just applying a library.

---

## ğŸ“‚ Whatâ€™s Inside

### Core Notebook
- **QLearning_GridWorld.nb** â€” Complete implementation of the grid-world navigation problem in Mathematica:  
  - Environment setup (states, obstacles, start, goal)  
  - Q-Learning algorithm (with Î±, Î³, Îµ tuning)  
  - Reward structure design  
  - Training loop and convergence tracking  
  - Visualization of the learned path  

### Notes & Documentation
- **q-learning-gridworld.pdf** â€” Short write-up covering:  
  - Reward shaping in small environments  
  - How Îµ-greedy exploration balances exploration vs exploitation  
  - Why convergence can be achieved in under 100 episodes in this setup  

---

## ğŸ“Œ Highlights
- Designing rewards that encourage efficient navigation  
- Deriving and applying the **Q-update rule** step by step  
- Watching the agent gradually â€œdiscoverâ€ the optimal route  
- Demonstrating convergence in <100 episodes  
- Using Mathematica as both a **symbolic tool** and a **simulation engine**  

---

## ğŸ¯ Who This Is For
If youâ€™re learning **reinforcement learning fundamentals**, want a compact example of **Q-learning in Mathematica**, or just curious about how simple agents can learn optimal paths in toy environments, this should help.  

Itâ€™s especially useful if you prefer step-by-step derivations and transparent code rather than just relying on Python RL libraries.

---

## ğŸ›  Tools Used
- **Mathematica** â€” for implementation, visualizations, and symbolic checks  
- **Custom reward structure** â€” designed for balancing goal-reaching, obstacle avoidance, and step minimization  

---

## ğŸ“¬ Contact
If youâ€™re exploring reinforcement learning, symbolic computation, or want to discuss grid-world experiments:  

ğŸ“§ **vimals24@iitk.ac.in**  

---

*"In small grids, big ideas grow â€” Q-learning shows how even the simplest agents can learn through trial, error, and persistence."*
